+++
tags  = []
title = "Link: Why AI and machine learning researchers are beginning to embrace PyTorch"
date  = "2017-08-04 22:21:24"
+++
<p>via <a href="https://www.oreilly.com/ideas/why-ai-and-machine-learning-researchers-are-beginning-to-embrace-pytorch">Why AI and machine learning researchers are beginning to embrace PyTorch - O'Reilly Media</a></p><p>Facebook, research は PyTorch で production は Caffe2 なのか。そしてむかし TF 登場以前 Torch に入門しようとした時はドキュメントがさっぱりわけわからなかったけれど、PyTorch は充実してるね。ぱっと<a href="http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html">チュートリアル</a>をながめたかんじ悪くなさそう。TF のグラフ定義言語と実行環境がまざって混乱する感じがない。これが imperative style API の良さというやつか。</p><p>Immediate mode の gradient はどう計算するかというと、<a href="http://pytorch.org/tutorials/beginner/former_torchies/autograd_tutorial.html">tape based automatic differentiation</a> というテクニックを使うらしい。具体的には gradient に参加させたい変数をオブジェクトでラップし、関数の側にもそのオブジェクトを識別するラッパーを用意し、オブジェクトに適用された関数というか計算を全部覚えておく("tape")。で、その記録された計算を逆にたどって微分する。計算を記録したあとは TF とかと同じということだろう。</p><p>PyTorch が引用しているもとねたの一つは <a href="https://github.com/hips/autograd">Autograd</a> という Python のライブラリ。これは numpy ベースで書かれた関数を自動微分できるという。これを Twitter が Torch (Lua) に<a href="https://github.com/twitter/torch-autograd">移植</a>し、それを更に PyTorch が取り込んだという紆余曲折。Autograd ライブラリもオリジナルというわけではない。<a href="http://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/">この解説記事</a>からリンクされた <a href="http://www.autodiff.org/">autodiff.org</a> の<a href="http://www.autodiff.org/?module=Introduction&amp;submenu=Selected%20Books">参考文献</a>をみると歴史あるジャンルであることが伺える。科学者えらい。</p><p>などと NN  framework の dynamic と static の違いがわかってよかった。ついでに TF が numpy 互換っぽい API になっているのも自動微分の流儀なのだと理解した。</p><p><hr /></p><p>追記。<a href="https://github.com/pytorch/pytorch/releases/tag/v0.2.0">PyTorch 0.2</a> のリリースノートを見てみる。</p><p>二階微分ができる。しかも近似ではないっぽい。つまり ops が自分の二階微分を提供しないといけない。まじか。大変じゃね？しかしおかげでむずかしい GAN が実装できるらしい。</p><p>分散トレーニング。どうもスクリプトを書くノードに送りつけて実行するスタイル(MPI 系)ぽい。まじか・・・。</p><p>TensorFlow とは違う進化の仕方をしているなあ。遠目に見つつ、そのうち気が向いたら試してみよう。</p>
