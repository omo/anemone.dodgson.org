+++
tags  = ["mld"]
title = "MLD: Maximum (Log) Likelihood"
date  = "2017-04-21 07:53:37"
+++
<p><a href="http://www.deeplearningbook.org/contents/ml.html">Part 1, 5.5</a> あたり。</p><p>誤差を最小化するのではなく、モデル(確率モデル)上でのデータの likelihood を最大化する、という形で学習を定義すると色々捗るという話。</p><p>Mean squared error も実際はガウス分布の likelihood と等価であるとか、よくわかってなかったので読み直す。ついでに cross-entropy も等価だとかいいだすし。何度か読み直し、Python でグラフを書いてみたりもして、ようやくメンタルモデルができてきた。</p><p>Bayesian 方面から来るとこの方が馴染みやすいのかもしれないけど、何かと確率変数で物事を定式化していくスタイルはなかなか馴染めなずしんどい。ガチガチに高階関数なコードに馴染めないのと似ている。なんか「おまえらが Rx だなんだと有難がっているものは全部モナドだ！バーン！」みたいな気分。もうちょっと適応したいもんです。Monad も Bayes も。</p><p>&nbsp;</p>
