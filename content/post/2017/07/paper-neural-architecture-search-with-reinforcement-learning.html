+++
tags  = ["mld"]
title = "Paper: Neural Architecture Search with Reinforcement Learning"
date  = "2017-07-30 21:09:40"
+++
<p>via <a href="https://research.google.com/pubs/pub45826.html">Neural Architecture Search with Reinforcement Learning</a></p><p>話題になっている論文を読んで全然わからず撃墜されようシリーズ。</p><p>これはまあ表題にあるような話で、具体的には LSTM で CNN や RNN のアーキテクチャを生成し、その性能を実際に train して求めては reinforcement leraning で探索する、というものらしい。よりまともな説明は <a href="https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html">Research Blog: Using Machine Learning to Explore Neural Network Architecture</a> などを見ると良い。</p><p>NN で RNN/CNN のアーキテクチャを探索(生成)するというのが面白いところなのだと思う。うまい具合に解空間を絞っておいてあげないと探索しきれないので、そのへんのさじ加減が妙。</p><p>それはさておき自分はそもそも reinforcement learning が全然わかってない。gradient が計算できなくても "policy gradient" というのが計算できればよい、らしいが、なんだそりゃ。総当りや greedy とも違うのだろうし、なんだかまったく謎。Reinforcement learning なんてロボットとかやってる人だけのニッチかと思っていたら、こうして汎用的な目的でも使えるのだなあ。きっとそのうち勉強しないといけないものなのでしょう。そんなもののリストはひたすら伸びていくばかり・・・。</p><p>あとぎょっとしたのはトレーニングに 600 GPU も使っているというところ。NN 関係、分散できそうで案外できないように見えていたけれど、そんなにスケールできるとなるといよいよでかい企業の独擅場になってしまいそう。ちょっと前に FAIR に <a href="https://code.facebook.com/posts/260115687797006/accelerating-machine-learning-for-computer-vision/">CNN をガっと 200 GPU くらい使って並列化する話</a>があったのを思い出す。まあ独占的にでかいデータセットを使う話ではなくあくまでアルゴリズムの範疇なので、外野でも再現性はあるといえばある。AWS でやるとか。しかし金のかかる世界であることには違いない。</p><p>続編にあたる <a href="https://arxiv.org/abs/1707.07012v1">[1707.07012v1] Learning Transferable Architectures for Scalable Image Recognition</a> では、前の論文で発見したアーキテクチャ（を改良したもの）を積み重ねて使ったら探索に使った以外の、より大きなのデータセットでもいい結果がでたよ、という話。Neural Search でみつけた architecture が transferable なのだとしたら（そして transferable ってのは割と信憑性あるよね。結局実践の場ではどこかの researchers が手動で発見し論文に書いた architecture を再利用しているわけだから。) もう国家予算で 10000GPU くらいかき集めて最強の architecture でも探せばいいのでは、という気になってくる。天文学者が望遠鏡つくるみたいな・・・。</p><p><hr /></p><p>と、わからないなりに SF ぽさを楽しめる部分もあるね、NN 論文。楽しいという以上の効能はさておき。</p>
