+++
tags  = ["mld"]
title = "MLD: Goodfellow, Wrapping Up"
date  = "2017-06-28 14:07:25"
+++
<p><a href="http://www.deeplearningbook.org/">Deep Learning 本</a>, 最後の 3 章は挫折。理由を考えるに、自分は Structure Probabilistic Modeling がわかってないのでその知識を前提に話をされてもわからん、ということだと思う。16 章でちらっと説明があるわけだけれど、こんな駆け足でわかるなら苦労しないです・・・</p><p>20 章 <a href="http://www.deeplearningbook.org/contents/generative_models.html">Deep Generating Models</a> をわからないなりに冷やかして把握したこととして、generative models には undirected graph に基づくものすなわち RBM/DBM などと、directed graph にもとづくものすなわち GAN とか VAE とかがある。そして RBM 勢は世間のニュースを観ている限りいまいち盛り上がってない。ので undirected graph ベースの structured modeling はがんばって勉強しなくても当面はいいのではなかろうか。</p><p>一方で今をときめく GAN とかは directed graph ベースのモデルだとされており、つまり Bayes net とか標準的な PGM の延長にあるっぽい。したがって graphical model 自体は勉強しないとダメそう。GAN や VAE が実際に graphical model と言えるのかはよくわからないけれど、結局 generative models は確率変数になんかする話であり、確率ベースの方法論である PGM にはそのための道具立てが揃っている、ということなのだろうなあ。きっと。<a href="http://edwardlib.org/">Edward</a> みたいのもあることだし、NN と PGM は最終的には両方やんないとダメなのでしょう・・・。</p><p><hr /></p><p>本全体の感想。なかなかよかった。特に Part 2 の実践編がよかった。NN の基礎的かつ理論的な部分ではだいぶ理解が進んだと思う。Part 3 は自分にはまだ難しすぎたけれど、それでも representation learning の章は面白かった。目次を除くと 700  ページくらいのわりかし薄い本な点も助けになった気がする。これ以上厚いと辛いだろうね。</p><p>一方で薄いせいではしょられている部分も多いから、二版が出るなら間違いなく厚まりそう。だからこういう本は薄い版のうちから読むのがお得と <a href="https://www.amazon.com/Real-Time-Rendering-Third-Tomas-Akenine-Moller/dp/1568814240">RTR</a> (from 500 pages at 1st ed. to 1000 pages at 3rd ed.) から学んだ。まあ改版しなそうな予感もあるけれど。このゴールドラッシュの最中によく書いたよねこの本。エライ。</p><p><hr /></p><p>このあとどうしましょう。</p><p>まずはいくつか復習したいことなどが溜まってきたので、そのために必要なものを読む予定。これはすぐおわる（希望的観測）。</p><p>あといいかげんコードを書きたい。TensorFlow, 公式ドキュメントがやや不親切で再挑戦は気が重いなあ・・・とおもいつつウェブを物色していたら O'Reilly から <a href="http://shop.oreilly.com/product/0636920063698.do">Learning TensorFlow</a> という本がベータ版として出版されかかっており、ちらっと中身をみたかんじ自分には割と良さそう。機械学習とは・・・とかいう話をすっとばして TF の説明だけしている。おかげで薄い。これを読もうかな。<a href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a> も Amazon のレビューを見る限り良さそうだけれど、自分は Scikit-Learn 成分は摂取済みなのだった。</p><p>そのあとは <a href="http://cs231n.stanford.edu/">cs231n</a> なり <a href="http://web.stanford.edu/class/cs224n/">cs224n</a> なりで画像処理なり自然言語処理との組み合わせを勉強したい気もするけれど、そんな気力が湧くかは不明。ひげぽん氏みたいに自分でデータを集めてなんかやるのも楽しそうだし。ぬるく TF 入門を済ませてから考えることにする。</p><p>Generative Models の勉強もそのうち再挑戦したいけれど、しばらくは放置かな。確率や PGM など generative models の前に勉強しないといけないことが多く、数式をかき分け厚い教科書たちを読み進める気力が湧くまでは無理。しかもそれらを読んでる間はほとんど手を動かせない。まあ generative models がわからなくてもできることは色々あるでしょう、きっと。</p><p><hr /></p><p>ところで generative models はどうやって勉強すればいいのかとウェブを冷やかしていたらみつけた <a href="https://www.cs.toronto.edu/~duvenaud/courses/csc2541/index.html">U. Tronto の授業</a>、シラバスに「最後のプロジェクトは NIPS に論文書きます(誇張)」とか書いてあってフいた。さすが AI のメッカはちがうな。</p>
