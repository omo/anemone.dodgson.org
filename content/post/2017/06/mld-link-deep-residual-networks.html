+++
tags  = ["mld"]
title = "MLD: Link: Deep Residual Networks"
date  = "2017-06-14 08:16:59"
+++
<p>via <a href="http://kaiminghe.com/icml16tutorial/index.html">ICML 2016 Tutorial on Deep Residual Networks</a></p><p>別のところで出て来たための調べ物。</p><p><a href="https://arxiv.org/abs/1512.03385">[1512.03385] Deep Residual Learning for Image Recognition</a> が論文本体。上のスライドがよくかけていた。ConvNet も深くしていくと段々辛くなってくるけれど、residual cconnection というある種の skip connection をつけてやると 100 layer 以上いけるようになるよ、という話らしい。深いレイヤでおこりがちな gradient の vanish/explode を線形の成分をまぜてやることで緩和するという点で LSTM に似ているという話がどこかに書いてあった、が、リンク失念。こういうわかりやすいアーキテクチャは心休まるな・・・。</p><p>&nbsp;</p>
