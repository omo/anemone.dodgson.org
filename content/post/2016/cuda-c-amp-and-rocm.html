+++
tags  = []
title = "CUDA, C++ AMP and ROCm"
date  = "2016-12-12 17:51:07"
+++
<p>Neural Networks の実装はなぜか軒並み CUDA を使っている。自分も数日前から CUDA の入門書を読みはじめた。以前 OpenCL の入門書を読んだことがあるのだけれど、GPGPU へのアプローチにどんな違いがあるのか興味が湧いたため。</p><p>そんな矢先、AMD が GPGPU 用のカード <a href="http://radeon.com/en-us/instinct/">Radeon Instinct </a>をリリースしたというニュースがあった。彼らが提供するプログラマ向けのプラットホームは Radeon Open Compute Platform (<a href="https://github.com/RadeonOpenCompute">ROCm</a>) と銘打たれ GitHub に置かれている。色々なツールがあるけれど、主要なコンポーネントは HCC というコンパイラらしい。このコンパイラから使える API は数年前に Microsoft が発表した <a href="https://msdn.microsoft.com/en-us/magazine/hh882446.aspx">C++ AMP</a> と互換だそうな。</p><p>C++AMP や HPP の<a href="https://gist.github.com/scchan/540d410456e3e2682dbf018d3c179008">サンプルコード</a>を眺めてみると、ホストのコードと GPU でうごくカーネルのコードがわかれていない。これは CUDA も同じ。OpenCL ではカーネルを別のファイルに定義し、そのカーネルからコンパイルしたバイナリを OpenCL のホスト API でロードし GPU に送る、みたいな荒々しい API だった。C++ AMP だと C++ の lambda を専用のアルゴリズム関数(並列 for みたいなやつら)に渡すだけ。あとはツールチェインがよろしくやってくれる。こりゃいいわ。</p><p>CUDA は C++ ではなく C なので若干切り口は違うけれども、やはりホストとカーネルが混在できる。このアプローチだと GPU 業者はコンパイラをまるごと提供する必要がある。ROCm は Clang を使っている。ROCm にしろ CUDA の NVCC にしろ、基本的には ホスト/カーネル混在のコードをCPU 用のコンパイラに渡す前で前処理し、カーネル部分だけ自分でコンパイルするかんじらしい。これは OpenCL よりだいぶいい。特に C++AMP の lambda を使えるところ。未来っぽい。</p><p>CUDA は今の所 lambda は使えないけれど、対応は時間の問題に見える。CUDA 自体は C API ながら NVCC は C++ も扱え、NVIDIA 自身 <a href="http://docs.nvidia.com/cuda/thrust/">Thrust</a> という CUDA 向け C++ ライブラリを提供している。</p><p>なお NN まわりでの CUDA の普及は CUDA それ自体の良さだけでなく、NVIDIA 謹製の <a href="https://developer.nvidia.com/cudnn">cuDNN</a> なるライブラリが職人的チューニングですごい速く、みなこれを使っているからという面もある<a href="https://news.ycombinator.com/item?id=13159943">そうな</a>。NVIDIA がんばった。積み重ねを感じる。OpenCL を勉強している場合じゃなかった。</p><p>&nbsp;</p>
